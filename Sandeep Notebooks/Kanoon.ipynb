{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from os.path import join as p_join\n",
    "from os import listdir\n",
    "# scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "court_names = ['delhidc', 'jodhpur', 'uttaranchal', 'gauhati', 'kolkata_app', 'allahabad', \n",
    "               'karnataka', 'kolkata', 'srinagar', 'punjab', 'himachal_pradesh', 'madhyapradesh',\n",
    "               'andhra', 'meghalaya', 'chattisgarh', 'bangaloredc', 'kerala', 'jammu', 'scorders',\n",
    "               'chennai', 'jharkhand', 'orissa', 'sikkim', 'telangana', 'patna', 'rajasthan',\n",
    "               'delhi', 'supremecourt', 'patna_orders', 'gujarat', 'bombay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global dir_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spiders_dir = 'kanoon_spiders/kanoon_spiders/spiders'\n",
    "aux_dir = 'data/auxillary_files/'\n",
    "case_list_dir = 'data/raw_data/case_list_data/'\n",
    "case_data_dir = 'data/raw_data/case_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return [a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "def get_soup(url):\n",
    "    # Fetch the response from the url as a BeautifulSoup element\n",
    "    return BeautifulSoup(requests.get(url).text)\n",
    "\n",
    "def get_num_results(url):\n",
    "    # From a given case list url determine the number of results available\n",
    "    soup = get_soup(url)\n",
    "    res = soup.find('div', attrs={'class':'results_middle'}).find('b')\n",
    "    if res.text=='No matching results':\n",
    "        return 0\n",
    "    else:\n",
    "        return int(res.text.split()[-1])\n",
    "    \n",
    "def split_time_interval(start, end, n=4):   \n",
    "    # helper function to split a date interval into smaller intervals 'appropriately'. \n",
    "    # returns list of intervals with a start and an end date\n",
    "\n",
    "    intervals = []\n",
    "    if type(start)==str:\n",
    "        start = datetime.strptime(start, '%Y-%m-%d').date()\n",
    "        end = datetime.strptime(end, '%Y-%m-%d').date()\n",
    "        \n",
    "    if (end-start).days>n+1:\n",
    "\n",
    "        intv = timedelta(days = ((end-start)//n).days)\n",
    "\n",
    "        for i in range(n):\n",
    "            intervals.append((start+(i*intv),start+((i+1)*intv)-timedelta(1)))\n",
    "\n",
    "        if start+((i+1)*intv)-timedelta(1)!=end:\n",
    "            intervals.append((start+((i+1)*intv), end))    \n",
    "    else:\n",
    "        intervals.extend([(start+timedelta(i), start+timedelta(i)) for i in range(1+(end-start).days)])   \n",
    "    return intervals\n",
    "\n",
    "\n",
    "def make_start_url_list_for_spiders(url_list, num_pieces):\n",
    "    pieces = split(url_list, num_pieces)\n",
    "    for i, piece in enumerate(pieces):\n",
    "        with open(p_join(spiders_dir, 'start_urls','piece_'+str(i+1)+'.txt'), 'w') as f:            \n",
    "            f.write(str(piece))\n",
    "\n",
    "\n",
    "def get_list_of_all_downloaded_cases():\n",
    "    '''returns list of kanoon_ids of scraped_cases'''\n",
    "    all_scraped_ids = []\n",
    "    for root, dirs, files in os.walk(case_data_dir):\n",
    "        all_scraped_ids.extend([int(file.split('.')[0]) for file in files])\n",
    "    return all_scraped_ids\n",
    "\n",
    "def extract_result_ids_from_case_list_page(fpath):\n",
    "    with open(fpath, 'r') as f:\n",
    "        d = BeautifulSoup(f.read())\n",
    "    try:\n",
    "        return [x.find('a')['href'].split('/')[2] for x in d.find_all('div', \n",
    "                                                                      attrs={'class':'result_title'})]\n",
    "    except:\n",
    "        return \n",
    "        \n",
    "def total_num_files(fpath):\n",
    "    num_cases = 0\n",
    "    for root, dirs, files in os.walk(f_path, topdown=True):\n",
    "        num_cases+=len(files)\n",
    "    return num_cases\n",
    "\n",
    "def fname_to_url_converter(fname, return_params=False):\n",
    "    \n",
    "    court = re.match(r'[A-Za-z_]+', fname)[0][:-1]\n",
    "    page = fname.split('_')[-1].split('.')[0]\n",
    "    ordering = fname.split('_')[-2]\n",
    "    end = fname.split('_')[-3]\n",
    "    start = fname.split('_')[-4]    \n",
    "    \n",
    "    if 'recent' in ordering:\n",
    "        url  =''.join(['https://indiankanoon.org/search/?formInput=doctypes%3A%20',\n",
    "                      court, '%20fromdate%3A%20', start, '%20todate%3A%20', end,\n",
    "                      '%20sortby%3A%20', ordering, '&pagenum=', page])\n",
    "    else:\n",
    "        url = ''.join(['https://indiankanoon.org/search/?formInput=doctypes%3A%20', \n",
    "                       court, '%20fromdate%3A%20', start, '%20todate%3A%20', end, '&pagenum=', page])\n",
    "        \n",
    "    if return_params:\n",
    "        return url, (court, start, end, ordering, page)\n",
    "    else:\n",
    "        return url\n",
    "    \n",
    "def make_fname(url, data_type):\n",
    "    data_type = data_type\n",
    "    url = url\n",
    "    if data_type == 'case_list_data':\n",
    "        u = url.split('%20')\n",
    "        court, start_date, end_date, page_num = u[1], u[3], u[5].split('&')[0], u[-1].split('=')[-1]\n",
    "        sort_method = [s for s in ['mostrecent', 'leastrecent'] if s in url]\n",
    "        if sort_method:\n",
    "            sort_by = sort_method[0]\n",
    "        else:\n",
    "            sort_by = 'relevant'\n",
    "        return '_'.join([court, start_date, end_date, sort_by, page_num]) + '.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: For any court get valid date ranges and the number of cases in each range\n",
    "-------------------\n",
    "\n",
    "1. For any tuple (court, start, end, num_cases), valid means either:\n",
    " \n",
    " 1. num_cases<=800 or\n",
    " \n",
    " 2. num cases>800 and start = end\n",
    " \n",
    "2. We use 800 because there is a limit of 400 accessible per day and we can get 800 by changing the ordering of the cases from most recent to least recent and obtain a reliably a bare minimum of 800 cases.  \n",
    " \n",
    "2. We fix the date full date range to be 01-01-1900 to 31-12-2019\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_url_from_tuple(court, start, end): \n",
    "    \n",
    "    # given the date range and court get the link to access the case list    \n",
    "    if type(start)==type(datetime.strptime('1900-01-01', '%Y-%m-%d').date()):\n",
    "        start = datetime.strftime(start, '%d-%m-%Y')\n",
    "        end = datetime.strftime(end, '%d-%m-%Y')\n",
    "    \n",
    "    url = ''.join(['https://indiankanoon.org/search/?formInput=doctypes:', court,\n",
    "                   '%20fromdate:', start, '%20todate:%20', end])\n",
    "    return url\n",
    "\n",
    "def fname_to_url_converter(fname):\n",
    "    \n",
    "    court = re.match(r'[A-Za-z_]+', fname)[0][:-1]\n",
    "    page = fname.split('_')[-1].split('.')[0]\n",
    "    ordering = fname.split('_')[-2]\n",
    "    end = fname.split('_')[-3]\n",
    "    start = fname.split('_')[-4]\n",
    "    if 'recent' in ordering:\n",
    "        url  =''.join('https://indiankanoon.org/search/?formInput=doctypes%3A%20', court,\n",
    "                      '%20fromdate%3A%20', start, '%20todate%3A%20', end,\n",
    "                      '%20sortby%3A%20', ordering, '&pagenum=', page)\n",
    "    else:\n",
    "        url = ''.join('https://indiankanoon.org/search/?formInput=doctypes%3A%20', court,\n",
    "                      '%20fromdate%3A%20', start, '%20todate%3A%20', end, '&pagenum=', page)\n",
    "    return url\n",
    "    \n",
    "\n",
    "def split_invalid_range_into_pieces(court, start, end, num_cases):\n",
    "    # returns list of valid tuples covering the input date range\n",
    "\n",
    "    large_urls = [(start, end, num_cases)]\n",
    "    small_urls = []\n",
    "    \n",
    "    while large_urls:\n",
    "        for item in large_urls:\n",
    "\n",
    "            large_urls.remove(item)\n",
    "            intervals = split_time_interval(item[0], item[1])\n",
    "            for interval in intervals:\n",
    "                print('large_urls: %d, small_urls: %d' %(len(large_urls), len(small_urls)))\n",
    "\n",
    "                m = get_num_results(make_url_from_tuple(court, interval[0], interval[1]))\n",
    "                \n",
    "                sleep(3)\n",
    "                if (m<=800):\n",
    "                    small_urls.append(interval+(m,))\n",
    "                elif (m>800) and (interval[0]==interval[1]):\n",
    "                    small_urls.append(interval+(m,))\n",
    "                else:\n",
    "                    large_urls.append(interval+(m,))\n",
    "                    \n",
    "    return [(court,)+x for x in small_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage:\n",
    "1. Get a dataframe of valid date ranges and associated number of cases\n",
    "2. Loop over court names to get the relevant dfs for all courts. \n",
    "3. If looping over multiple courts then use groupby and loop over the groups to write to file \n",
    "4. Write these dataframes to CSV files (or append to existing files) in 'data/auxillary_files/court_start_end_num_cases_num_available'\n",
    "4. Note the column titled 'num_available' in the CSV files are to be appended later once we have the case_lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "court='sikkim'\n",
    "Start, End = datetime.strptime('1900-01-01', '%Y-%m-%d'), datetime.strptime('2019-12-31', '%Y-%m-%d')\n",
    "n = get_num_results(make_url_from_tuple(court, Start, End))\n",
    "l = split_invalid_range_into_pieces(court, Start, End, n)\n",
    "df = Pd.DataFrame(l, columns = 'court start end num_cases'.split())\n",
    "\n",
    "relevant_aux_dir = p_join(aux_dir, 'court_start_end_num_cases_num_available')\n",
    "\n",
    "# Write the data to  a CSV file\n",
    "if os.path.exists(p_join(relevant_aux_dir, court+'.csv')):\n",
    "    df_old = pd.read_csv(p_join(relevant_aux_dir, court+'.csv'))\n",
    "    pd.concat([df_old, concat]).to_csv(p_join(relevant_aux_dir, court+'.csv'), index=False)\n",
    "else:\n",
    "    df.to_csv(p_join(relevant_aux_dir, court+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (i): Get case list\n",
    "--------------------------------\n",
    "\n",
    "1. For a given tuple (court, start, end, num_cases) get an associated list of urls; one for each page and order type (least recent, most recent or relevant). \n",
    "2. This list can be fed into scrapy\n",
    "3. The scraped files are then moved from 'kanoon_spiders/kanoon_spiders/spiders/case_list_data' to the appropriate folders. Here, the appropriate folder means the associated court folder in 'data/raw_data/case_list_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_case_list_page_urls(court, start, end, num_cases):\n",
    "    '''\n",
    "    helper function to make a list of valied urls with orderings and page_nums\n",
    "    to get the case list with given parameters\n",
    "    '''\n",
    "    try:\n",
    "        start = str(start.date())\n",
    "        end = str(end.date())\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if len(start.split('-')[0])==4:\n",
    "        start = '-'.join(start.split('-')[::-1])\n",
    "        end = '-'.join(end.split('-')[::-1])\n",
    "\n",
    "    output_url_list = []\n",
    "    \n",
    "    if not num_cases:\n",
    "        return output_url_list\n",
    "    \n",
    "    pieces = ['https://indiankanoon.org/search/?formInput=doctypes%3A%20',\n",
    "                '%20fromdate%3A%20','%20todate%3A%20','%20sortby%3A%20','&pagenum=']\n",
    "    order = ['leastrecent', 'mostrecent']\n",
    "    \n",
    "    if num_cases > 800:\n",
    "        \n",
    "        output_url_list.extend([pieces[0]+court+pieces[1]+start+pieces[2]+end+pieces[3]\n",
    "                                +order[0]+pieces[4]+str(i) for i in range(40)])\n",
    "        \n",
    "        output_url_list.extend([pieces[0]+court+pieces[1]+start+pieces[2]+end+pieces[3]\n",
    "                                +order[1]+pieces[4]+str(i) for i in range(40)])  \n",
    "        \n",
    "        output_url_list.extend([pieces[0]+court+pieces[1]+start+pieces[2]+end\n",
    "                                +pieces[4]+str(i) for i in range(40)])\n",
    "        \n",
    "    elif 400<num_cases<=800:\n",
    "        n = (num_cases-400+10)//10\n",
    "\n",
    "        output_url_list.extend([pieces[0]+court+pieces[1]+start+pieces[2]+end+pieces[3]\n",
    "                                +order[0]+pieces[4]+str(i) for i in range(40)])\n",
    "        \n",
    "        output_url_list.extend([pieces[0]+court+pieces[1]+start+pieces[2]+end+pieces[3]\n",
    "                                +order[1]+pieces[4]+str(i) for i in range(n)])\n",
    "        \n",
    "    else:\n",
    "        n = (num_cases+10)//10\n",
    "        \n",
    "        output_url_list.extend([pieces[0]+court+pieces[1]+start+pieces[2]+end+pieces[3]\n",
    "                                +order[0]+pieces[4]+str(i) for i in range(n)])      \n",
    "        # populate a list of urls which would yield lists of kanoon_ids whihc are then \n",
    "        \n",
    "    return output_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_list_urls = []\n",
    "for param_list in tqdm(df[['court, start', 'end', 'num_cases']].values.tolist()):\n",
    "    case_list_urls.extend(make_start_url_list_for_spiders(*param_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate a list of urls which would yield lists of kanoon_ids whihc are then \n",
    "case_list_urls = []\n",
    "for param_list in tqdm(df[['court, start', 'end', 'num_cases']].values.tolist()):\n",
    "    case_list_urls.extend(make_start_url_list_for_spiders(*param_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (ii): Download the data\n",
    "----------------------\n",
    "1. write the files to be scraped in the format piece_i.txt and place them in 'kanoon_spiders/kanoon_spiders/spiders/start_urls'\n",
    "\n",
    "2. Use gen_spider.py located in the spiders folder to make n spiders using the 'case_list_data' parameter for data_type\n",
    "\n",
    "3. Use the bash script to run the spiders quickly.\n",
    "    \n",
    "4. **Note**: make the bash script executable by running *chmod +x bash.sh* and then ./bash.sh from the folder containing the script\n",
    "\n",
    "5. The case lists would download to 'kanoon_spiders/kanoon_spiders/spiders/case_list_data'\n",
    "\n",
    "6. Once the cases are scraped, move the downloaded files to the appropriate court folder in 'data/raw_data/case_list_data' using the move_data function with 'case_list_data' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make num_spider number of start_url lists for spiders in kanoon_spiders\n",
    "num_spiders = 100\n",
    "make_start_url_list_for_spiders(case_list_urls, num_spiders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the bash script located at 'kanoon_spiders/kanoon_spiders/spiders/bash_IK_session.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (iii): Extract Kanoon_ids from the downloaded cases into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7318/7318 [01:53<00:00, 64.53it/s]\n"
     ]
    }
   ],
   "source": [
    "case_urls = []\n",
    "for file in tqdm(listdir(p_join(spiders_dir,'case_list_data'))):\n",
    "    f_path = p_join(spiders_dir,'case_list_data', file)\n",
    "    case_urls.extend(['https://indiankanoon.org/doc/'+k_id+'/' for k_id\n",
    "                      in extract_result_ids_from_case_list_page(f_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refresh the list of all scraped_ids\n",
    "\n",
    "all_scraped_ids = set(get_list_of_all_downloaded_cases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the case_urls list\n",
    "\n",
    "case_urls = [url for url in case_urls if int(re.search(r'[0-9]+', url)[0]) not in all_scraped_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of case_ids available to be scraped: %d' %len(case_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (iv): Move and sort the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_data(data_type, court_year_dict = {}):\n",
    "    '''\n",
    "    Helper function to move the downloaded case list files (or downloaded cases) to the appropriate \n",
    "    data folders  \n",
    "    '''\n",
    "    err, num_moved = [], 0\n",
    "   \n",
    "    dest_folder='data/raw_data'\n",
    "    src_folder = spiders_dir\n",
    "    \n",
    "    if data_type == 'case_list_data':\n",
    "        src_folder = p_join(src_folder, 'case_list_data')\n",
    "        dest_folder = p_join(dest_folder, 'case_list_data')\n",
    "        all_files = listdir(src_folder)\n",
    "        for file in all_files:\n",
    "            try:\n",
    "\n",
    "                court = re.search(r'[A-Za-z_]+', file)[0][:-1]\n",
    "                shutil.move(p_join(src_folder, file), \n",
    "                            p_join(dest_folder, court, file))\n",
    "                num_moved+=1\n",
    "            except:\n",
    "                err.append(file)\n",
    "\n",
    "    elif data_type == 'case_data':       \n",
    "        \n",
    "        src_folder = p_join(src_folder, 'case_data')\n",
    "        dest_folder = p_join(dest_folder, 'case_data')\n",
    "        all_files = listdir(src_folder)\n",
    "        \n",
    "        for file in all_files:\n",
    "            try:          \n",
    "                court, year = court_year_dict[int(file.split('.')[0])]\n",
    "                if not os.path.exists(p_join(dest_folder, court, year)):\n",
    "                    os.makedirs(p_join(dest_folder, court, year))\n",
    "\n",
    "                    \n",
    "                shutil.move(p_join(src_folder, file),\n",
    "                            p_join(dest_folder, court, year, file))\n",
    "                num_moved+=1\n",
    "            except:\n",
    "                pass\n",
    "    print('Number of cases moved: %d' %num_moved)\n",
    "    print('Number of cases not moved: %d' %len(listdir(src_folder)))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases moved: 0\n",
      "Number of cases not moved: 0\n"
     ]
    }
   ],
   "source": [
    "# This moves the data to folders in 'data/raw_data/case_list_data/'\n",
    "errors = move_data('case_list_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (i): Update the case_urls list and scrape cases\n",
    "--------------------------\n",
    "\n",
    "1. Use the new start urls with the case_data spiders.\n",
    "2. The same bash script from the earlier step can be used to start the spiders\n",
    "3. The case files (with file name in the format kanoon_id.txt) will be downloaded to 'kanoon_spiders/kanoon_spiders/spiders/case_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_start_url_list_for_spiders(case_urls, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 (ii): Move the cases to the data folder\n",
    "------------------------\n",
    "1. There are two ways to move the downloaded case files from 'kanoon_spiders/kanoon_spiders/spiders/case_data' to the appropriate folder in 'data/raw_data/case_data':\n",
    "    1. *Load the dict (id_court_year.pkl, if it has been updated with the new info) required to move the scraped cases to their respective folders (required if using the move_data method)*, or\n",
    "    2. Use the move_individual_case_file method (using the court_names_dict.pkl dict) to move them individually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case individual files have to be moved and the source is not known use\n",
    "def move_individual_case_file(f_path):\n",
    "    \n",
    "    with open(f_path, 'r') as f:\n",
    "        d = BeautifulSoup(f.read())\n",
    "    court = d.find('div', attrs = {'class':'docsource_main'})\n",
    "    title = d.find_all('div', attrs={'class':'doc_title'})[-1].text\n",
    "    \n",
    "    year = datetime.strptime(title.split(' on ')[-1], '%d %B, %Y').date().year\n",
    "    court_name, year = court_names_dict[court.text], str(year)\n",
    "    dest_path = p_join(case_data_dir, court_name, year)\n",
    "    \n",
    "    shutil.move(f_path, dest_path)\n",
    "    #print(f_path, dest_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled dict for use with the move_data method\n",
    "with open(p_join(aux_dir, 'id_court_year_dict.pkl'), 'rb') as f:\n",
    "    id_court_year_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the helper dict for converting the court names as they appear in the case text to court folder names\n",
    "with open(p_join(aux_dir, 'court_names_dict.pkl'), 'rb') as f:\n",
    "    court_names_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cases moved: 0\n",
      "Number of cases not moved: 29990\n"
     ]
    }
   ],
   "source": [
    "#errors = move_data('case_data', id_court_year_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29990/29990 [02:58<00:00, 167.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm([p_join(spiders_dir, 'case_data', x) \n",
    "                  for x in listdir(p_join(spiders_dir, 'case_data'))]):\n",
    "    try:\n",
    "        move_individual_case_file(file)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Stats and other auxillary files\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Extract *kanoon id, title and date* of cases from the scraped case lists\n",
    "-------------------------------------------\n",
    "\n",
    "1. Store basic metadata of the cases (from the case_list files) into CSV files by court in 'data/auxillary_files/id_court_title_date'\n",
    "2. Use this data to form a dict of kanoon_ids as keys and (court, year) as values -- which is useful for sorting the cases when downloaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_id_date_from_case_list_data(court_name):\n",
    "    '''\n",
    "    use court_name exactly as in the raw_data/case_list_data for consistency\n",
    "    Writes the parsed data to file in the auxillary files folder\n",
    "    '''    \n",
    "    errors = []\n",
    "    cols = 'kanoon_Id, court, title, judgment_date'.split()\n",
    "    df_title_date = pd.DataFrame(columns = cols)\n",
    "    court_folder = listdir(p_join(case_list_dir, court_name))        \n",
    "\n",
    "    for i, file in enumerate(court_folder):\n",
    "        try:\n",
    "            if i%5000==0:\n",
    "                \n",
    "                print('Working on %d out of %d' %(i+1, len(court_folder)))\n",
    "                print('Num errors: %d' %len(errors))\n",
    "                print('Num cases extracted: %d' %df_title_date.shape[0])\n",
    "\n",
    "            with open(p_join(case_list_dir, court_name, file), 'r') as f:\n",
    "                d = f.read().replace('\\n', '')\n",
    "            res_list = []\n",
    "            \n",
    "            for item in d.split(r'<div class=\"result_title\">')[1:]:\n",
    "                title = item.split('>')[1][:-3]\n",
    "                kanoon_id = item.split('href=\"/docfragment/')[1].split('/')[0]\n",
    "                jud_date = datetime.strptime(title.split(' on ')[-1].strip(), '%d %B, %Y').date()\n",
    "                res_list.append((kanoon_id, court_name, title, jud_date))\n",
    "            \n",
    "            df_title_date = pd.concat([df_title_date, pd.DataFrame(res_list, columns = cols)])\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "            \n",
    "        except:\n",
    "            errors.append((court_name, file))\n",
    "            \n",
    "    # write remaining files to CSV\n",
    "    df_title_date.to_csv(p_join(aux_dir, 'id_court_title_date', \n",
    "                              court_name+'_case_title_and_date.csv'), index = False)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Usage\n",
    "1. err = extract_title_id_date_from_case_list_data('patna') will parse the patna case_list_data\n",
    "2. err is the set of errors encountered while parsing\n",
    "3. Gujarat is has about 70 errors since in those files the titles have encoding errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = extract_title_id_date_from_case_list_data(court)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A (i): Make dict with kanoon_ids as keys and  a tuple of court and year as values and populate a list of all kanoon_ids from case_lists\n",
    "------------------------------------------\n",
    "1. **Note**: This is a duplication of the above CSV data as a dict\n",
    "2. There is a difference between all_kanoon_ids_from_case_lists and all_kanoon_ids -- as there may a be cases scraped from other ad-hoc methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    id_court_year_dict\n",
    "except NameError:\n",
    "    id_court_year_dict = {}\n",
    "    \n",
    "all_kanoon_ids_from_case_lists = set([])\n",
    "for court in tqdm(os.listdir(p_join(aux_dir, 'id_court_title_date'))):\n",
    "    df = pd.read_csv(p_join(aux_dir,'id_court_title_date', court))\n",
    "    d_keys = df['kanoon_Id,']\n",
    "    d_values = list(zip(df['court,'], df['judgment_date'].map(lambda x: x.split('-')[0])))\n",
    "    d = dict(zip(d_keys, d_values))\n",
    "    all_kanoon_ids_from_case_lists.update(d_keys)\n",
    "    \n",
    "    id_court_year_dict = {**id_court_year_dict, **d} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the result for future use\n",
    "\n",
    "with open(p_join(aux_dir, 'id_court_year_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(id_court_year_dict, f)\n",
    "    \n",
    "with open(p_join(aux_dir, 'all_kanoon_ids_from_case_lists.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_kanoon_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled files\n",
    "\n",
    "with open(p_join(aux_dir, 'id_court_year_dict.pkl'), 'rb') as f:\n",
    "    id_court_year_dict = pickle.load(f)\n",
    "    \n",
    "with open(p_join(aux_dir, 'all_kanoon_ids.pkl'), 'rb') as f:\n",
    "    all_kanoon_ids_from_case_lists = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of kanoon_ids available: %d' %len(all_kanoon_ids_from_case_lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A (iii): Get a list of the kanoon_ids of all cases scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kanoon_ids = set(get_list_of_all_downloaded_cases())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p_join(aux_dir, 'all_kanoon_ids.pkl'), 'wb') as f:\n",
    "    pickle.dump(all_kanoon_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B: Update the number of cases available column (out of the num_cases number of cases hosted on India Kanoon)\n",
    "--------------\n",
    " Since there is a restriction on the number of cases returned (given the date range parameters), we can potentially only scrape a fraction of the cases. On days when there are more than 400 cases -- we stand to lose out on getting all the num_cases number of cases -- we call num_available the number of cases actually returned (those for which we can get the kanoon_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [5:24:59<00:00, 629.00s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Loop takes extremely long to run (can be improved by using regex instead of parsing with BeautifulSoup)\n",
    "court_date_range_ids_dict = {}\n",
    "\n",
    "for court in tqdm(court_names):\n",
    "    for root, dirs, files in os.walk(p_join(case_list_dir, court)):\n",
    "        if files:\n",
    "            for file in files:\n",
    "                court, start, end = fname_to_url_converter(file, return_params=True)[1][:3]          \n",
    "                k_ids = set(extract_result_ids_from_case_list_page(p_join(root, file)))\n",
    "                if (court, start, end) not in court_date_range_ids_dict:\n",
    "                    court_date_range_ids_dict[(court, start, end)] = k_ids\n",
    "                else:\n",
    "                    court_date_range_ids_dict[(court, start, end)].update(k_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(p_join(aux_dir, 'court_date_range_ids_dict.pkl'), 'wb') as f:\n",
    "    pickle.dump(court_date_range_ids_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:05<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for court in tqdm(court_names):\n",
    "    \n",
    "    df_court = pd.read_csv(p_join(aux_dir, 'court_start_end_num_cases_num_available', court+'.csv'))\n",
    "    l = []\n",
    "    for i in range(df_court.shape[0]):\n",
    "        court, start, end, num_cases = tuple(df_court.iloc[i].values.tolist()[:4])\n",
    "        try:\n",
    "            num = len(court_date_range_ids_dict[(court,\n",
    "                                                 '-'.join(start.split('-')[::-1]),\n",
    "                                                 '-'.join(end.split('-')[::-1]))])\n",
    "            l.append(int(num))\n",
    "        except:\n",
    "            if num_cases==0:\n",
    "                l.append(int(0))\n",
    "            else:\n",
    "                l.append(None)\n",
    "    df_court['num_available'] = l\n",
    "    df_court.to_csv(p_join(aux_dir, 'court_start_end_num_cases_num_available', court+'.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scraped_case_info = []\n",
    "for court in court_names:\n",
    "    fpath = p_join(case_data_dir, court)\n",
    "    scraped_case_info.append((court, total_num_files(fpath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>court</th>\n",
       "      <th>num_scraped_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delhidc</td>\n",
       "      <td>323773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jodhpur</td>\n",
       "      <td>242597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uttaranchal</td>\n",
       "      <td>106172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gauhati</td>\n",
       "      <td>67721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kolkata_app</td>\n",
       "      <td>619704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>allahabad</td>\n",
       "      <td>261793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>karnataka</td>\n",
       "      <td>517803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kolkata</td>\n",
       "      <td>256638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>srinagar</td>\n",
       "      <td>18483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>punjab</td>\n",
       "      <td>808597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>himachal_pradesh</td>\n",
       "      <td>134150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>madhyapradesh</td>\n",
       "      <td>835236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>andhra</td>\n",
       "      <td>27030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meghalaya</td>\n",
       "      <td>19302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chattisgarh</td>\n",
       "      <td>59756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>bangaloredc</td>\n",
       "      <td>68282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>kerala</td>\n",
       "      <td>445725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jammu</td>\n",
       "      <td>38165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>scorders</td>\n",
       "      <td>255718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chennai</td>\n",
       "      <td>519819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jharkhand</td>\n",
       "      <td>341639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>orissa</td>\n",
       "      <td>50218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sikkim</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>telangana</td>\n",
       "      <td>67906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>patna</td>\n",
       "      <td>86298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>rajasthan</td>\n",
       "      <td>295392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>delhi</td>\n",
       "      <td>111032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>supremecourt</td>\n",
       "      <td>51551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>patna_orders</td>\n",
       "      <td>1225668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gujarat</td>\n",
       "      <td>1078524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bombay</td>\n",
       "      <td>252244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               court  num_scraped_cases\n",
       "0            delhidc             323773\n",
       "1            jodhpur             242597\n",
       "2        uttaranchal             106172\n",
       "3            gauhati              67721\n",
       "4        kolkata_app             619704\n",
       "5          allahabad             261793\n",
       "6          karnataka             517803\n",
       "7            kolkata             256638\n",
       "8           srinagar              18483\n",
       "9             punjab             808597\n",
       "10  himachal_pradesh             134150\n",
       "11     madhyapradesh             835236\n",
       "12            andhra              27030\n",
       "13         meghalaya              19302\n",
       "14       chattisgarh              59756\n",
       "15       bangaloredc              68282\n",
       "16            kerala             445725\n",
       "17             jammu              38165\n",
       "18          scorders             255718\n",
       "19           chennai             519819\n",
       "20         jharkhand             341639\n",
       "21            orissa              50218\n",
       "22            sikkim                143\n",
       "23         telangana              67906\n",
       "24             patna              86298\n",
       "25         rajasthan             295392\n",
       "26             delhi             111032\n",
       "27      supremecourt              51551\n",
       "28      patna_orders            1225668\n",
       "29           gujarat            1078524\n",
       "30            bombay             252244"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scraped_case_info, columns = ['court', 'num_scraped_cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of scraped cases: 9187079\n"
     ]
    }
   ],
   "source": [
    "print('Total number of scraped cases: %d' %total_num_files('data/raw_data/case_data/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 (Tests):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-Do\n",
    "---------------------------\n",
    "1. Write a function to generate the bash script -- taking the number of spiders as a parameter\n",
    "2. Find ways of adding more parameters to the date ranges where there are more than 400 cases to extract more cases \n",
    "3. Loop through all the case data in the data folder and check if they are 'valid', i.e., try and extract case_title and some other content form the 'judgment' tag and rescrape those cases where there is an error\n",
    "4. Check why there are less cases than expected in date ranges where there are less than 400 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IK_venv",
   "language": "python",
   "name": "ik_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
